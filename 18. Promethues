ubuntu@ip-172-31-64-155:~$ helm uninstall prometheus -n monitoring
release "prometheus" uninstalled
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
No resources found in monitoring namespace.
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pvc -n monitoring
No resources found in monitoring namespace.
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pv -n monitoring
No resources found
ubuntu@ip-172-31-64-155:~$ helm install prometheus prometheus-community/prometheus -n monitoring
NAME: prometheus
LAST DEPLOYED: Sun Dec 15 04:25:14 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           0/1     Pending   0          3s
prometheus-kube-state-metrics-88947546-hgm7z        0/1     Running   0          4s
prometheus-prometheus-node-exporter-9rfb5           1/1     Running   0          4s
prometheus-prometheus-pushgateway-9f8c968d6-8k4d8   0/1     Running   0          4s
prometheus-server-6b884dc7f6-www2t                  0/2     Pending   0          4s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pv -n monitoring
No resources found
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pvc -n monitoring
NAME                                STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
prometheus-server                   Pending                                                     <unset>                 22s
storage-prometheus-alertmanager-0   Pending                                                     <unset>                 22s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl describe pod prometheus-alertmanager-0 -n monitoring
Name:             prometheus-alertmanager-0
Namespace:        monitoring
Priority:         0
Service Account:  prometheus-alertmanager
Node:             <none>
Labels:           app.kubernetes.io/instance=prometheus
                  app.kubernetes.io/name=alertmanager
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=prometheus-alertmanager-5f46576954
                  statefulset.kubernetes.io/pod-name=prometheus-alertmanager-0
Annotations:      checksum/config: aebb4052a77bcfda4219bae6cb5938b3a29c8a109c1add1359d359a1af577bf5
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    StatefulSet/prometheus-alertmanager
Containers:
  alertmanager:
    Image:      quay.io/prometheus/alertmanager:v0.27.0
    Port:       9093/TCP
    Host Port:  0/TCP
    Args:
      --storage.path=/alertmanager
      --config.file=/etc/alertmanager/alertmanager.yml
    Liveness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from storage (rw)
      /etc/alertmanager from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v9xr2 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  storage-prometheus-alertmanager-0
    ReadOnly:   false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-alertmanager
    Optional:  false
  kube-api-access-v9xr2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  52s   default-scheduler  0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
ubuntu@ip-172-31-64-155:~$ sudo vi prometheus-server-pv.yaml
ubuntu@ip-172-31-64-155:~$ sudo vi alert-manger-pv.yaml
ubuntu@ip-172-31-64-155:~$ microk8s kubectl apply -f prometheus-server-pv.yaml -n monitoring
persistentvolume/prometheus-server-pv created
ubuntu@ip-172-31-64-155:~$ microk8s kubectl apply -f alert-manger-pv.yaml -n monitoring
persistentvolume/prometheus-alert-manager-pv created
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS              RESTARTS   AGE
prometheus-alertmanager-0                           0/1     Pending             0          5m18s
prometheus-kube-state-metrics-88947546-hgm7z        1/1     Running             0          5m19s
prometheus-prometheus-node-exporter-9rfb5           1/1     Running             0          5m19s
prometheus-prometheus-pushgateway-9f8c968d6-8k4d8   1/1     Running             0          5m19s
prometheus-server-6b884dc7f6-www2t                  0/2     ContainerCreating   0          5m19s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS             RESTARTS     AGE
prometheus-alertmanager-0                           0/1     Pending            0            5m23s
prometheus-kube-state-metrics-88947546-hgm7z        1/1     Running            0            5m24s
prometheus-prometheus-node-exporter-9rfb5           1/1     Running            0            5m24s
prometheus-prometheus-pushgateway-9f8c968d6-8k4d8   1/1     Running            0            5m24s
prometheus-server-6b884dc7f6-www2t                  1/2     CrashLoopBackOff   1 (2s ago)   5m24s
ubuntu@ip-172-31-64-155:~$ 
ubuntu@ip-172-31-64-155:~$ sudo mkdir -p /mnt/data/prometheus
ubuntu@ip-172-31-64-155:~$ sudo chown -R 1000:1000 /mnt/data/prometheus
ubuntu@ip-172-31-64-155:~$ sudo chmod -R 755 /mnt/data/prometheus
ubuntu@ip-172-31-64-155:~$ sudo ls -l /mnt/data/prometheus
total 0
ubuntu@ip-172-31-64-155:~$ sudo chown -R 65534:65534 /mnt/data/prometheus
ubuntu@ip-172-31-64-155:~$ sudo chmod -R 775 /mnt/data/prometheus
ubuntu@ip-172-31-64-155:~$ microk8s kubectl delete pvc prometheus-server-6b884dc7f6-www2t -n monitoring
Error from server (NotFound): persistentvolumeclaims "prometheus-server-6b884dc7f6-www2t" not found
ubuntu@ip-172-31-64-155:~$ microk8s kubectl delete pod prometheus-server-6b884dc7f6-www2t -n monitoring
pod "prometheus-server-6b884dc7f6-www2t" deleted
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pvc -n monitoring
NAME                                STATUS   VOLUME                        CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
prometheus-server                   Bound    prometheus-server-pv          8Gi        RWO                           <unset>                 11m
storage-prometheus-alertmanager-0   Bound    prometheus-alert-manager-pv   2Gi        RWO                           <unset>                 11m
ubuntu@ip-172-31-64-155:~$ microk8s kubectl delete  pvc prometheus-server  -n monitoring
persistentvolumeclaim "prometheus-server" deleted


^Z
[3]+  Stopped                 microk8s kubectl delete pvc prometheus-server -n monitoring
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pvc -n monitoring
NAME                                STATUS        VOLUME                        CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
prometheus-server                   Terminating   prometheus-server-pv          8Gi        RWO                           <unset>                 12m
storage-prometheus-alertmanager-0   Bound         prometheus-alert-manager-pv   2Gi        RWO                           <unset>                 12m
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          12m
prometheus-kube-state-metrics-88947546-hgm7z        1/1     Running   0          12m
prometheus-prometheus-node-exporter-9rfb5           1/1     Running   0          12m
prometheus-prometheus-pushgateway-9f8c968d6-8k4d8   1/1     Running   0          12m
prometheus-server-6b884dc7f6-wjbxn                  2/2     Running   0          3m8s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.33    <none>        9093/TCP   15m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   15m
prometheus-kube-state-metrics         ClusterIP   10.152.183.158   <none>        8080/TCP   15m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.60    <none>        9100/TCP   15m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.220   <none>        9091/TCP   15m
prometheus-server                     ClusterIP   10.152.183.181   <none>        80/TCP     15m
ubuntu@ip-172-31-64-155:~$ 
ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.33    <none>        9093/TCP       18m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       18m
prometheus-kube-state-metrics         ClusterIP   10.152.183.158   <none>        8080/TCP       18m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.60    <none>        9100/TCP       18m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.220   <none>        9091/TCP       18m
prometheus-server                     NodePort    10.152.183.181   <none>        80:30826/TCP   18m
ubuntu@ip-172-31-64-155:~$ 


apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus-server-pv
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: prometheus
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  hostPath:
    path: /mnt/data/prometheus



apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus-alert-manager-pv
  labels:
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/name: alertmanager
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data/alertmanager







    scrape_configs:
      - job_name: my-react-nginx-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: react-nginx-namespace #change namespace name to your preferred one
      

ubuntu@ip-172-31-64-155:~$ helm show values prometheus-community/prometheus -n monitoring > promotheus-values.yaml
ubuntu@ip-172-31-64-155:~$ sudo vi promotheus-values.yaml
ubuntu@ip-172-31-64-155:~$ sudo vi promotheus-values.yaml
ubuntu@ip-172-31-64-155:~$ helm upgrade prometheus prometheus-community/prometheus -f promotheus-values.yaml -n monitoring
Error: failed to parse promotheus-values.yaml: error converting YAML to JSON: yaml: line 799: block sequence entries are not allowed in this context
ubuntu@ip-172-31-64-155:~$ sudo vi promotheus-values.yaml
ubuntu@ip-172-31-64-155:~$ helm upgrade prometheus prometheus-community/prometheus -f promotheus-values.yaml -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Sun Dec 15 07:20:00 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local




ubuntu@ip-172-31-64-155:~$ microk8s kubectl delete pods --all -n monitoring
pod "prometheus-alertmanager-0" deleted
pod "prometheus-kube-state-metrics-88947546-55xnj" deleted
pod "prometheus-prometheus-node-exporter-lq465" deleted
pod "prometheus-prometheus-pushgateway-9f8c968d6-nqk8w" deleted
pod "prometheus-server-6b884dc7f6-qs94b" deleted
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          5s
prometheus-kube-state-metrics-88947546-j4z92        0/1     Running   0          8s
prometheus-prometheus-node-exporter-9457t           1/1     Running   0          6s
prometheus-prometheus-pushgateway-9f8c968d6-pb4wj   0/1     Running   0          8s
prometheus-server-6b884dc7f6-wqfk9                  1/2     Running   0          8s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.183   <none>        9093/TCP   7m16s
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   7m16s
prometheus-kube-state-metrics         ClusterIP   10.152.183.63    <none>        8080/TCP   7m16s
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.175   <none>        9100/TCP   7m16s
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.88    <none>        9091/TCP   7m16s
prometheus-server                     ClusterIP   10.152.183.99    <none>        80/TCP     7m16s
ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit svc prometheus-server -n monitoring
Edit cancelled, no changes made.
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.183   <none>        9093/TCP       7m58s
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       7m58s
prometheus-kube-state-metrics         ClusterIP   10.152.183.63    <none>        8080/TCP       7m58s
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.175   <none>        9100/TCP       7m58s
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.88    <none>        9091/TCP       7m58s
prometheus-server                     NodePort    10.152.183.99    <none>        80:32322/TCP   7m58s
ubuntu@ip-172-31-64-155:~$ 













ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit deploy react-chart -n react-nginx-namespace
deployment.apps/react-chart edited
ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit deploy react-chart -n react-nginx-namespace
deployment.apps/react-chart edited

###add these annotations to deployment
##testing on release
    prometheus.io/path: /metrics
    prometheus.io/port: "9113"
    prometheus.io/scrape: "true"


ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n react-nginx-namespace
NAME                           READY   STATUS    RESTARTS   AGE
react-chart-6f5958b467-dj2x7   2/2     Running   0          35s
react-chart-6f5958b467-njtkw   2/2     Running   0          38s
ubuntu@ip-172-31-64-155:~$ 

ubuntu@ip-172-31-64-155:~$ helm upgrade prometheus prometheus-community/prometheus -f promotheus-values.yaml -n monitoring
Error: failed to parse promotheus-values.yaml: error converting YAML to JSON: yaml: line 813: could not find expected ':'
ubuntu@ip-172-31-64-155:~$ sudo vi promotheus-values.yaml
##add this side car container for deployment
- name: nginx-exporter
  image: nginx/nginx-prometheus-exporter:latest
  args:
    - -nginx.scrape-uri=http://localhost:80/stub_status
  ports:
    - containerPort: 9113
      name: metrics
ubuntu@ip-172-31-64-155:~$ helm upgrade prometheus prometheus-community/prometheus -f promotheus-values.yaml -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Sun Dec 15 09:39:08 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local

ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.183   <none>        9093/TCP   105m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   105m
prometheus-kube-state-metrics         ClusterIP   10.152.183.63    <none>        8080/TCP   105m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.175   <none>        9100/TCP   105m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.88    <none>        9091/TCP   105m
prometheus-server                     ClusterIP   10.152.183.99    <none>        80/TCP     105m
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          98m
prometheus-kube-state-metrics-88947546-j4z92        1/1     Running   0          98m
prometheus-prometheus-node-exporter-9457t           1/1     Running   0          98m
prometheus-prometheus-pushgateway-9f8c968d6-pb4wj   1/1     Running   0          98m
prometheus-server-6b884dc7f6-wqfk9                  2/2     Running   0          98m
ubuntu@ip-172-31-64-155:~$ microk8s kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-64-155:~$ microk8s kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.183   <none>        9093/TCP       107m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       107m
prometheus-kube-state-metrics         ClusterIP   10.152.183.63    <none>        8080/TCP       107m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.175   <none>        9100/TCP       107m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.88    <none>        9091/TCP       107m
prometheus-server                     NodePort    10.152.183.99    <none>        80:30503/TCP   107m





###add these annotations to deployment
##testing on release
    prometheus.io/path: /metrics
    prometheus.io/port: "9113"
    prometheus.io/scrape: "true"
##add this side car container for deployment
- name: nginx-exporter
  image: nginx/nginx-prometheus-exporter:latest
  args:
    - -nginx.scrape-uri=http://localhost:80/stub_status
  ports:
    - containerPort: 9113
      name: metrics
##supporting blog if you wish to learn more - https://sysdig.com/blog/monitor-nginx-prometheus/




 - job_name: react-nginx-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
      # Keep only pods in the 'react-nginx' namespace
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: react-nginx-namespace
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__meta_kubernetes_pod_ip]
            action: replace
            replacement: '$1:9113'
            target_label: __address__





From scracth:
======================


microk8s (1.31/stable) v1.31.3 from Canonical✓ installed
ubuntu@ip-172-31-85-30:~$ sudo usermod -a -G microk8s ubuntu
ubuntu@ip-172-31-85-30:~$ mkdir ~/.kube
ubuntu@ip-172-31-85-30:~$ sudo microk8s kubectl config view --raw > ~/.kube/config
ubuntu@ip-172-31-85-30:~$ sudo su - root
root@ip-172-31-85-30:~# sudo su - ubuntu
ubuntu@ip-172-31-85-30:~$ microk8s kubectl get all --all-namespaces
NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
kube-system   pod/calico-kube-controllers-759cd8b574-nfd2v   1/1     Running   0          2m13s
kube-system   pod/calico-node-qxpvv                          1/1     Running   0          2m14s
kube-system   pod/coredns-7896dbf49-pgkgg                    1/1     Running   0          2m13s

NAMESPACE     NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.152.183.1    <none>        443/TCP                  2m21s
kube-system   service/kube-dns     ClusterIP   10.152.183.10   <none>        53/UDP,53/TCP,9153/TCP   2m17s

NAMESPACE     NAME                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/calico-node   1         1         1       1            1           kubernetes.io/os=linux   2m16s

NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           2m16s
kube-system   deployment.apps/coredns                   1/1     1            1           2m17s

NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/calico-kube-controllers-759cd8b574   1         1         1       2m14s
kube-system   replicaset.apps/coredns-7896dbf49                    1         1         1       2m14s
ubuntu@ip-172-31-85-30:~$ alias kubectl="microk8s kubectl"
ubuntu@ip-172-31-85-30:~$ kubectl create ns react-nginx-namespace
namespace/react-nginx-namespace created
ubuntu@ip-172-31-85-30:~$ microk8s enable ingress
Infer repository core for addon ingress
Enabling Ingress
ingressclass.networking.k8s.io/public created
ingressclass.networking.k8s.io/nginx created
namespace/ingress created
serviceaccount/nginx-ingress-microk8s-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-microk8s-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-microk8s-role created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created
configmap/nginx-load-balancer-microk8s-conf created
configmap/nginx-ingress-tcp-microk8s-conf created
configmap/nginx-ingress-udp-microk8s-conf created
daemonset.apps/nginx-ingress-microk8s-controller created
Ingress is enabled
ubuntu@ip-172-31-85-30:~$ kubectl get all --all-namespaces
NAMESPACE     NAME                                           READY   STATUS              RESTARTS   AGE
ingress       pod/nginx-ingress-microk8s-controller-822xb    0/1     ContainerCreating   0          3s
kube-system   pod/calico-kube-controllers-759cd8b574-nfd2v   1/1     Running             0          3m51s
kube-system   pod/calico-node-qxpvv                          1/1     Running             0          3m52s
kube-system   pod/coredns-7896dbf49-pgkgg                    1/1     Running             0          3m51s

NAMESPACE     NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.152.183.1    <none>        443/TCP                  3m59s
kube-system   service/kube-dns     ClusterIP   10.152.183.10   <none>        53/UDP,53/TCP,9153/TCP   3m55s

NAMESPACE     NAME                                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
ingress       daemonset.apps/nginx-ingress-microk8s-controller   1         1         0       1            0           <none>                   3s
kube-system   daemonset.apps/calico-node                         1         1         1       1            1           kubernetes.io/os=linux   3m54s

NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           3m54s
kube-system   deployment.apps/coredns                   1/1     1            1           3m55s

NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/calico-kube-controllers-759cd8b574   1         1         1       3m52s
kube-system   replicaset.apps/coredns-7896dbf49                    1         1         1       3m52s
ubuntu@ip-172-31-85-30:~$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
Downloading https://get.helm.sh/helm-v3.16.4-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
helm installed into /usr/local/bin/helm
ubuntu@ip-172-31-85-30:~$ cd react-chart
-bash: cd: react-chart: No such file or directory
ubuntu@ip-172-31-85-30:~$ helm create react-chart
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/ubuntu/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/ubuntu/.kube/config
Creating react-chart
ubuntu@ip-172-31-85-30:~$ cd react-chart
ubuntu@ip-172-31-85-30:~/react-chart$ cd templates
ubuntu@ip-172-31-85-30:~/react-chart/templates$ cd 
ubuntu@ip-172-31-85-30:~$ cd react-chart
ubuntu@ip-172-31-85-30:~/react-chart$ ls
Chart.yaml  charts  templates  values.yaml
ubuntu@ip-172-31-85-30:~/react-chart$ rm -rf Chart.yaml templates  values.yaml
ubuntu@ip-172-31-85-30:~/react-chart$ ls
charts
ubuntu@ip-172-31-85-30:~/react-chart$ git clone https://github.com/akhilgit19/helmchart.git
Cloning into 'helmchart'...
remote: Enumerating objects: 24, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 24 (delta 6), reused 23 (delta 6), pack-reused 0 (from 0)
Receiving objects: 100% (24/24), 7.62 KiB | 1.90 MiB/s, done.
Resolving deltas: 100% (6/6), done.
ubuntu@ip-172-31-85-30:~/react-chart$ ls
charts  helmchart
ubuntu@ip-172-31-85-30:~/react-chart$ cd helmchart
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ ks
Command 'ks' not found, but can be installed with:
sudo apt install qdl
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ ls
Chart.yaml  README.md  templates  values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ chmod 600 ~/.kube/config
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm install react-chart .
NAME: react-chart
LAST DEPLOYED: Fri Jan  3 07:57:09 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  http://hkhnet.xyz/
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl create ns monitoring
namespace/monitoring created
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. ⎈Happy Helming!⎈
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm install prometheus prometheus-community/prometheus -n monitoring
NAME: prometheus
LAST DEPLOYED: Fri Jan  3 07:57:48 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           0/1     Pending   0          91s
prometheus-kube-state-metrics-88947546-bcptb        1/1     Running   0          92s
prometheus-prometheus-node-exporter-clfqt           1/1     Running   0          92s
prometheus-prometheus-pushgateway-9f8c968d6-96zhv   1/1     Running   0          92s
prometheus-server-6b884dc7f6-t6bvc                  0/2     Pending   0          92s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$     prometheus.io/path: /metrics
    prometheus.io/port: "9113"
    prometheus.io/scrape: "true"
-bash: prometheus.io/path:: No such file or directory
-bash: prometheus.io/port:: No such file or directory
-bash: prometheus.io/scrape:: No such file or directory
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo mkdir -p /mnt/data/prometheus
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo chmod 777 /mnt/data/prometheus
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo mkdir -p /mnt/data/alertmanager
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo chmod 777 /mnt/data/alertmanager
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi prometheus-server-pv.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi alert-manger-pv.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl apply -f prometheus-server-pv.yaml -n monitoring
persistentvolume/prometheus-server-pv created
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl apply -f alert-manger-pv.yaml -n monitoring
persistentvolume/prometheus-alert-manager-pv created
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pv -n monitoring
NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                          STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
prometheus-alert-manager-pv   2Gi        RWO            Retain           Bound    monitoring/storage-prometheus-alertmanager-0                  <unset>                          34s
prometheus-server-pv          8Gi        RWO            Retain           Bound    monitoring/prometheus-server                                  <unset>                          37s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pvc -n monitoring
NAME                                STATUS   VOLUME                        CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
prometheus-server                   Bound    prometheus-server-pv          8Gi        RWO                           <unset>                 4m22s
storage-prometheus-alertmanager-0   Bound    prometheus-alert-manager-pv   2Gi        RWO                           <unset>                 4m21s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          4m26s
prometheus-kube-state-metrics-88947546-bcptb        1/1     Running   0          4m27s
prometheus-prometheus-node-exporter-clfqt           1/1     Running   0          4m27s
prometheus-prometheus-pushgateway-9f8c968d6-96zhv   1/1     Running   0          4m27s
prometheus-server-6b884dc7f6-t6bvc                  1/2     Running   0          4m27s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP   4m46s
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   4m46s
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP   4m46s
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP   4m46s
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP   4m46s
prometheus-server                     ClusterIP   10.152.183.189   <none>        80/TCP     4m46s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP       5m22s
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       5m22s
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP       5m22s
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP       5m22s
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP       5m22s
prometheus-server                     NodePort    10.152.183.189   <none>        80:31086/TCP   5m22s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm show values prometheus-community/prometheus -n monitoring > promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ ls
Chart.yaml  README.md  alert-manger-pv.yaml  prometheus-server-pv.yaml  promo_values.yaml  templates  values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm upgrade prometheus prometheus-community/prometheus -f promo_values.yaml -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Fri Jan  3 08:06:36 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl delete pods --all -n monitoring 
pod "prometheus-alertmanager-0" deleted
pod "prometheus-kube-state-metrics-88947546-bcptb" deleted
pod "prometheus-prometheus-node-exporter-clfqt" deleted
pod "prometheus-prometheus-pushgateway-9f8c968d6-96zhv" deleted
pod "prometheus-server-6b884dc7f6-t6bvc" deleted
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          19s
prometheus-kube-state-metrics-88947546-hfrr6        1/1     Running   0          25s
prometheus-prometheus-node-exporter-qfjzv           1/1     Running   0          24s
prometheus-prometheus-pushgateway-9f8c968d6-p6qns   1/1     Running   0          25s
prometheus-server-6b884dc7f6-d7t2k                  1/2     Running   0          25s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP   10m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   10m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP   10m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP   10m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP   10m
prometheus-server                     ClusterIP   10.152.183.189   <none>        80/TCP     10m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP       10m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       10m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP       10m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP       10m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP       10m
prometheus-server                     NodePort    10.152.183.189   <none>        80:31492/TCP   10m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit deploy react-chart -n react-nginx-namespace
deployment.apps/react-chart edited
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit deploy react-chart -n react-nginx-namespace
Edit cancelled, no changes made.
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n react-nginx
No resources found in react-nginx namespace.
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n react-nginx-namespace
NAME                           READY   STATUS    RESTARTS   AGE
react-chart-698b7966bf-srglf   2/2     Running   0          2m1s
react-chart-698b7966bf-z2c5n   2/2     Running   0          2m4s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm upgrade prometheus prometheus-community/prometheus -f promo_values.yaml -n monitoring
Error: failed to parse promo_values.yaml: error converting YAML to JSON: yaml: line 796: did not find expected '-' indicator
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm upgrade prometheus prometheus-community/prometheus -f promo_values.yaml -n monitoring
Error: failed to parse promo_values.yaml: error converting YAML to JSON: yaml: line 796: did not find expected '-' indicator
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm upgrade prometheus prometheus-community/prometheus -f promo_values.yaml -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Fri Jan  3 08:22:19 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl delete pods --all -n monitoring 
pod "prometheus-alertmanager-0" deleted
pod "prometheus-kube-state-metrics-88947546-hfrr6" deleted
pod "prometheus-prometheus-node-exporter-qfjzv" deleted
pod "prometheus-prometheus-pushgateway-9f8c968d6-p6qns" deleted
pod "prometheus-server-6b884dc7f6-d7t2k" deleted
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          14s
prometheus-kube-state-metrics-88947546-jbg7h        1/1     Running   0          17s
prometheus-prometheus-node-exporter-72cwg           1/1     Running   0          15s
prometheus-prometheus-pushgateway-9f8c968d6-cxtls   0/1     Running   0          17s
prometheus-server-6b884dc7f6-r9q6w                  1/2     Running   0          17s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP   25m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   25m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP   25m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP   25m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP   25m
prometheus-server                     ClusterIP   10.152.183.189   <none>        80/TCP     25m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP       26m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       26m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP       26m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP       26m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP       26m
prometheus-server                     NodePort    10.152.183.189   <none>        80:31684/TCP   26m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubect get pods -n react-nginx-namespace
Command 'kubect' not found, did you mean:
  command 'kubectx' from deb kubectx (0.9.5-1ubuntu0.2)
Try: sudo apt install <deb name>
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n react-nginx-namespace
NAME                           READY   STATUS    RESTARTS   AGE
react-chart-698b7966bf-srglf   2/2     Running   0          9m54s
react-chart-698b7966bf-z2c5n   2/2     Running   0          9m57s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n react-nginx-namespace -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE              NOMINATED NODE   READINESS GATES
react-chart-698b7966bf-srglf   2/2     Running   0          10m   10.1.171.207   ip-172-31-85-30   <none>           <none>
react-chart-698b7966bf-z2c5n   2/2     Running   0          10m   10.1.171.206   ip-172-31-85-30   <none>           <none>
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ curl http://10.1.171.207:80
<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="Web site created using create-react-app"/><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,400;0,900;1,700&family=Pacifico&family=Roboto&family=Tilt+Warp&family=Ubuntu&display=swap" rel="stylesheet"><title>Gold App</title><style>*{font-family:Montserrat,sans-serif;font-family:Roboto,sans-serif}</style><script defer="defer" src="/static/js/main.009077a2.js"></script><link href="/static/css/main.cd48fc57.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ curl http://10.1.171.207:9113/metrics
# HELP go_gc_duration_seconds A summary of the wall-time pause (stop-the-world) duration in garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0.000129441
go_gc_duration_seconds{quantile="0.25"} 0.000129441
go_gc_duration_seconds{quantile="0.5"} 0.000206335
go_gc_duration_seconds{quantile="0.75"} 0.001018888
go_gc_duration_seconds{quantile="1"} 0.001018888
go_gc_duration_seconds_sum 0.001354664
go_gc_duration_seconds_count 3
# HELP go_gc_gogc_percent Heap size target percentage configured by the user, otherwise 100. This value is set by the GOGC environment variable, and the runtime/debug.SetGCPercent function. Sourced from /gc/gogc:percent
# TYPE go_gc_gogc_percent gauge
go_gc_gogc_percent 100
# HELP go_gc_gomemlimit_bytes Go runtime memory limit configured by the user, otherwise math.MaxInt64. This value is set by the GOMEMLIMIT environment variable, and the runtime/debug.SetMemoryLimit function. Sourced from /gc/gomemlimit:bytes
# TYPE go_gc_gomemlimit_bytes gauge
go_gc_gomemlimit_bytes 9.223372036854776e+18
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 11
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.23.3"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated in heap and currently in use. Equals to /memory/classes/heap/objects:bytes.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 2.178216e+06
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated in heap until now, even if released already. Equals to /gc/heap/allocs:bytes.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 4.721736e+06
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table. Equals to /memory/classes/profiling/buckets:bytes.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 4223
# HELP go_memstats_frees_total Total number of heap objects frees. Equals to /gc/heap/frees:objects + /gc/heap/tiny/allocs:objects.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 11524
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata. Equals to /memory/classes/metadata/other:bytes.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 2.67836e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and currently in use, same as go_memstats_alloc_bytes. Equals to /memory/classes/heap/objects:bytes.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 2.178216e+06
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used. Equals to /memory/classes/heap/released:bytes + /memory/classes/heap/free:bytes.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 4.34176e+06
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use. Equals to /memory/classes/heap/objects:bytes + /memory/classes/heap/unused:bytes
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 3.391488e+06
# HELP go_memstats_heap_objects Number of currently allocated objects. Equals to /gc/heap/objects:objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 2019
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS. Equals to /memory/classes/heap/released:bytes.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 4.317184e+06
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system. Equals to /memory/classes/heap/objects:bytes + /memory/classes/heap/unused:bytes + /memory/classes/heap/released:bytes + /memory/classes/heap/free:bytes.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 7.733248e+06
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 1.7358927221024482e+09
# HELP go_memstats_mallocs_total Total number of heap objects allocated, both live and gc-ed. Semantically a counter version for go_memstats_heap_objects gauge. Equals to /gc/heap/allocs:objects + /gc/heap/tiny/allocs:objects.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 13543
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures. Equals to /memory/classes/metadata/mcache/inuse:bytes.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 2400
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system. Equals to /memory/classes/metadata/mcache/inuse:bytes + /memory/classes/metadata/mcache/free:bytes.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 15600
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures. Equals to /memory/classes/metadata/mspan/inuse:bytes.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 43680
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system. Equals to /memory/classes/metadata/mspan/inuse:bytes + /memory/classes/metadata/mspan/free:bytes.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 48960
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place. Equals to /gc/heap/goal:bytes.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 4.194304e+06
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations. Equals to /memory/classes/other:bytes.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 389385
# HELP go_memstats_stack_inuse_bytes Number of bytes obtained from system for stack allocator in non-CGO environments. Equals to /memory/classes/heap/stacks:bytes.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 622592
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator. Equals to /memory/classes/heap/stacks:bytes + /memory/classes/os-stacks:bytes.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 622592
# HELP go_memstats_sys_bytes Number of bytes obtained from system. Equals to /memory/classes/total:byte.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 1.1492368e+07
# HELP go_sched_gomaxprocs_threads The current runtime.GOMAXPROCS setting, or the number of operating system threads that can execute user-level Go code simultaneously. Sourced from /sched/gomaxprocs:threads
# TYPE go_sched_gomaxprocs_threads gauge
go_sched_gomaxprocs_threads 2
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 8
# HELP nginx_exporter_build_info A metric with a constant '1' value labeled by version, revision, branch, goversion from which nginx_exporter was built, and the goos and goarch for the build.
# TYPE nginx_exporter_build_info gauge
nginx_exporter_build_info{branch="HEAD",goarch="amd64",goos="linux",goversion="go1.23.3",revision="eb921f6062ff814acf8913cade92def952dfbe2e",tags="unknown",version="1.4.0"} 1
# HELP nginx_up Status of the last metric scrape
# TYPE nginx_up gauge
nginx_up 0
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.05
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 65536
# HELP process_network_receive_bytes_total Number of bytes received by the process over the network.
# TYPE process_network_receive_bytes_total counter
process_network_receive_bytes_total 69278
# HELP process_network_transmit_bytes_total Number of bytes sent by the process over the network.
# TYPE process_network_transmit_bytes_total counter
process_network_transmit_bytes_total 200772
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 8
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.5040512e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.73589213839e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 1.267859456e+09
# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.
# TYPE process_virtual_memory_max_bytes gauge
process_virtual_memory_max_bytes 1.8446744073709552e+19
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 9
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ ls
Chart.yaml  README.md  alert-manger-pv.yaml  prometheus-server-pv.yaml  promo_values.yaml  templates  values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ sudo vi promo_values.yaml
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ helm upgrade prometheus prometheus-community/prometheus -f promo_values.yaml -n monitoring
Release "prometheus" has been upgraded. Happy Helming!
NAME: prometheus
LAST DEPLOYED: Fri Jan  3 08:29:23 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 4
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl delete pods --all -n monitoring 
pod "prometheus-alertmanager-0" deleted
pod "prometheus-kube-state-metrics-88947546-jbg7h" deleted
pod "prometheus-prometheus-node-exporter-72cwg" deleted
pod "prometheus-prometheus-pushgateway-9f8c968d6-cxtls" deleted
pod "prometheus-server-6b884dc7f6-r9q6w" deleted
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get pods -n monitoring
NAME                                                READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0                           1/1     Running   0          16s
prometheus-kube-state-metrics-88947546-8xk5n        1/1     Running   0          20s
prometheus-prometheus-node-exporter-h8mnk           1/1     Running   0          18s
prometheus-prometheus-pushgateway-9f8c968d6-zddgj   0/1     Running   0          20s
prometheus-server-6b884dc7f6-n5bv8                  1/2     Running   0          19s
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP   32m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP   32m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP   32m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP   32m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP   32m
prometheus-server                     ClusterIP   10.152.183.189   <none>        80/TCP     32m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl edit svc prometheus-server -n monitoring
service/prometheus-server edited
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ kubectl get svc -n monitoring
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
prometheus-alertmanager               ClusterIP   10.152.183.190   <none>        9093/TCP       32m
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP       32m
prometheus-kube-state-metrics         ClusterIP   10.152.183.37    <none>        8080/TCP       32m
prometheus-prometheus-node-exporter   ClusterIP   10.152.183.106   <none>        9100/TCP       32m
prometheus-prometheus-pushgateway     ClusterIP   10.152.183.50    <none>        9091/TCP       32m
prometheus-server                     NodePort    10.152.183.189   <none>        80:31902/TCP   32m
ubuntu@ip-172-31-85-30:~/react-chart/helmchart$ 


========================



IF YOU GET PERMISSION ERROR TO THE path for prometheus path:
=====================================================================
Step-by-Step Solution
Create the directory /mnt/data/prometheus:

Run the following command to create the directory:.

bash
Copy code
sudo mkdir -p /mnt/data/prometheus
Assign the correct ownership to the Prometheus user (usually user ID 1000 in Prometheus containers) and group:

bash
Copy code
sudo chown -R 1000:1000 /mnt/data/prometheus
Set the appropriate permissions for the directory:

bash
Copy code
sudo chmod -R 755 /mnt/data/prometheus
This ensures that Prometheus has read and write access to the directory.

Verify the directory's permissions:

To ensure that the permissions are set correctly, run:

bash
Copy code
sudo ls -l /mnt/data/prometheus
You should now see the directory with the correct permissions for user 1000 (or whichever user Prometheus runs as in the container).

5. Restart the Prometheus Pod
Once the directory is created and permissions are set, you should restart the Prometheus pod:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-qwf9b -n monitoring
Kubernetes will automatically recreate the pod, and Prometheus should now be able to write to the /mnt/data/prometheus directory.

6. Check Pod Logs
After the pod restarts, check the logs to confirm that the error no longer appears:

bash
Copy code
microk8s kubectl logs prometheus-server-5cf87687d7-qwf9b -n monitoring -c prometheus-server
You should see Prometheus starting up without any permission-related errors.

7. Verify the PV is Mounted Correctly
To make sure the PersistentVolume (PV) is mounted correctly in the pod, you can describe the pod again:

bash
Copy code
microk8s kubectl describe pod prometheus-server-5cf87687d7-qwf9b -n monitoring
Look for the storage-volume mount under the Mounts section, and ensure it is mapped to /data.


Step 1: Check Permissions of the Mounted Directory
Since /data is mapped to the PersistentVolume, we need to ensure that the prometheus container has the correct permissions to write to the /mnt/data/prometheus directory, where the volume is mounted on the host machine.

Check the directory permissions on the host for /mnt/data/prometheus (which should be the directory being mounted by Prometheus).

Run:

bash
Copy code
sudo ls -ld /mnt/data/prometheus
Ensure that the directory has appropriate permissions. The directory should be owned by the Prometheus container's user (which is typically UID 1000).

If it’s not owned by the correct user, update the ownership:

bash
Copy code
sudo chown -R 1000:1000 /mnt/data/prometheus
sudo chmod -R 755 /mnt/data/prometheus
Confirm permissions inside the pod:

If needed, you can also exec into the Prometheus pod and check the permissions inside the container:

bash
Copy code
microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- bash
Then check the permissions of the /data directory:

bash
Copy code
ls -ld /data
Ensure the permissions are adequate for the Prometheus container (it should be writeable by the container's user).

Step 2: Restart the Pod
Once the permissions are fixed, you can restart the Prometheus pod to see if the issue is resolved:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-t5mk5 -n monitoring
This will force Kubernetes to recreate the pod, hopefully with the corrected permissions.

Step 3: Check Logs After Restart
After restarting the pod, check the logs again to confirm that Prometheus starts up properly:

bash
Copy code
microk8s kubectl logs prometheus-server-5cf87687d7-t5mk5 -n monitoring -c prometheus-server
Let me know what you find! If the issue persists, we can dig deeper.


You said:
ubuntu@ip-172-31-82-231:~$ sudo ls -ld /mnt/data/prometheus
drwxr-xr-x 2 ubuntu ubuntu 4096 Dec 12 18:03 /mnt/data/prometheus
ubuntu@ip-172-31-82-231:~$ sudo chown -R 1000:1000 /mnt/data/prometheus
ubuntu@ip-172-31-82-231:~$ sudo chmod -R 755 /mnt/data/prometheus
ubuntu@ip-172-31-82-231:~$ microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- bash
Defaulted container "prometheus-server-configmap-reload" out of: prometheus-server-configmap-reload, prometheus-server
error: Internal error occurred: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "e8e787eaaa088844a6b9295cee6d8fc0ed2917c431165f77bc0bea1fafeddaa8": OCI runtime exec failed: exec failed: unable to start container process: exec: "bash": executable file not found in $PATH: unknown
ChatGPT said:
ChatGPT
It looks like the bash shell isn't available in the Prometheus container, which is quite common for minimal container images. Instead, you can try using sh (the shell usually available in such containers).

Step 1: Try using sh to exec into the container
Run the following command:

bash
Copy code
microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- sh
This will give you a shell inside the container. Once inside, you can check the permissions on the /data directory:

sh
Copy code
ls -ld /data
Step 2: Check Logs After Fixing Permissions
After ensuring the permissions are fixed, you can delete the pod again to force a restart and check the logs afterward:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-t5mk5 -n monitoring
Then, check the logs again:

bash
Copy code
microk8s kubectl logs prometheus-server-5cf87687d7-t5mk5 -n monitoring -c prometheus-server
Let me know if this works or if there are any further issues!


You said:
ubuntu@ip-172-31-82-231:~$ microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- sh
Defaulted container "prometheus-server-configmap-reload" out of: prometheus-server-configmap-reload, prometheus-server
/ $ 
/ $ ls -ld /data
ls: /data: No such file or directory
ChatGPT said:
ChatGPT
It looks like the /data directory doesn't exist inside the container. This could explain the permission issues you're seeing, as Prometheus is trying to access /data (the mount point for persistent storage) but the directory doesn't exist.

Step 1: Create the /data directory inside the container
To resolve this, you can manually create the /data directory inside the container. While inside the container, run:

sh
Copy code
mkdir -p /data
Then, set the appropriate permissions (similar to what was done on the host):

sh
Copy code
chmod -R 755 /data
Step 2: Check if Prometheus Server Starts
Now, exit the shell (exit), and then delete the pod to restart it:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-t5mk5 -n monitoring
After the pod restarts, check the logs again:

bash
Copy code
microk8s kubectl logs prometheus-server-5cf87687d7-t5mk5 -n monitoring -c prometheus-server
This should either resolve the issue or provide additional insights into any remaining problems. Let me know how it goes!


You said:
ubuntu@ip-172-31-82-231:~$ microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- sh
Defaulted container "prometheus-server-configmap-reload" out of: prometheus-server-configmap-reload, prometheus-server
/ $ 
/ $ mkdir -p /data
mkdir: can't create directory '/data': Permission denied
/ $ 
/ $ sudo mkdir -p /data
sh: sudo: not found
/ $ 
ChatGPT said:
ChatGPT
It seems that the sh shell inside the container doesn't have sudo installed, and you don't have sufficient permissions to create directories in /data directly. This is a common scenario in containers where processes typically run as non-root users for security reasons.

To address this issue, you can follow these steps:

Step 1: Fix the Permissions on the Persistent Volume
Since the /data directory is mounted from a persistent volume, the issue likely lies with the permissions of the directory on the host filesystem.

We need to ensure that the prometheus container has the proper permissions on the /mnt/data/prometheus directory on the host, as this is what is being mounted into the container.

Check the permissions of /mnt/data/prometheus again:

The user ID of the prometheus container needs to match the ownership of the /mnt/data/prometheus directory on the host.
Run this command again on the host to confirm:

bash
Copy code
ls -ld /mnt/data/prometheus
Ensure the directory has proper ownership and permissions:

We will set the correct permissions and ensure it’s owned by the prometheus container’s user ID.
The Prometheus container usually runs with the UID 65534 (which is the nobody user on many Linux systems), but you can verify this by inspecting the container’s image or checking the Kubernetes deployment.

To ensure the correct permissions:

bash
Copy code
sudo chown -R 65534:65534 /mnt/data/prometheus
sudo chmod -R 775 /mnt/data/prometheus
Restart the Pod to Apply Changes: Once the permissions are corrected on the host, restart the Prometheus pod:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-t5mk5 -n monitoring
Check the logs again: After the pod restarts, check the logs again to confirm if Prometheus can now write to /data:

bash
Copy code
microk8s kubectl logs prometheus-server-5cf87687d7-t5mk5 -n monitoring -c prometheus-server
Step 2: Verify Permissions Inside the Container
You can also try the following:

Re-enter the container shell after the pod restarts:

bash
Copy code
microk8s kubectl exec -it prometheus-server-5cf87687d7-t5mk5 -n monitoring -- sh
Check the /data directory:

bash
Copy code
ls -ld /data
If the directory is created successfully, Prometheus should be able to write to it, and the issue should be resolved.

Let me know if the permissions fix worked or if there are any other issues!


You said:
ubuntu@ip-172-31-82-231:~$ microk8s kubectl exec -it prometheus-server-5cf87687d7-tv5p2 -n monitoring -- sh
Defaulted container "prometheus-server-configmap-reload" out of: prometheus-server-configmap-reload, prometheus-server
/ $ 
/ $ ls -ld /data
ls: /data: No such file or directory
/ $ 
/ $ ls
bin   dev   etc   home  lib   proc  root  sys   tmp   usr   var
/ $ cd mnt
sh: cd: can't cd to mnt: No such file or directory
/ $ 
ChatGPT said:
ChatGPT
It seems that inside the Prometheus container, the /data directory does not exist, and you're unable to navigate to /mnt either. This confirms that the volume mounting might not have been set up correctly or is not being recognized inside the container.

To troubleshoot and resolve this, let's go through the following steps:

Step 1: Confirm the Volume Mount in the Pod Configuration
First, let's verify that the PersistentVolume (PV) and PersistentVolumeClaim (PVC) are properly mounted to the container.

Inspect the Pod definition for the volume mounts: Run the following command to check if the volume (/data) is mounted correctly in the container.

bash
Copy code
microk8s kubectl describe pod prometheus-server-5cf87687d7-tv5p2 -n monitoring
Look for the Volumes section in the output. You should see something like this:

yaml
Copy code
Volumes:
  storage-volume:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  prometheus-server
    ReadOnly:   false
In addition, the Mounts section for the container should show something like this:

yaml
Copy code
Mounts:
  /data from storage-volume (rw)
Step 2: Confirm Volume Mount Path in the Container
If the volume is mounted correctly in the configuration, we need to ensure that the container is actually accessing the correct path.

Re-check the volume mount in the container shell:

If the volume is mounted correctly, you should be able to see /data inside the container. Try the following again, now that we have confirmed the pod’s volume configuration:

bash
Copy code
microk8s kubectl exec -it prometheus-server-5cf87687d7-tv5p2 -n monitoring -- sh
Once inside the container, list the contents of /:

sh
Copy code
ls /
If you see a /data directory now, you can go ahead and check its permissions:

sh
Copy code
ls -ld /data
Step 3: Check the PV and PVC
If the volume is still not mounted inside the container, it could be an issue with the PersistentVolume (PV) and PersistentVolumeClaim (PVC) configuration. Let’s inspect those:

Check the PVC:

Run this to ensure the PVC is correctly bound to the PV:

bash
Copy code
microk8s kubectl get pvc prometheus-server -n monitoring
Ensure that the STATUS is Bound.

Check the PV:

Run this to check the PV configuration:

bash
Copy code
microk8s kubectl get pv prometheus-server-pv
Ensure that the PV's STATUS is Bound and that the CAPACITY and ACCESS MODES are correctly set up (e.g., 8Gi and RWO for ReadWriteOnce).

Step 4: Recreate the Pod if Necessary
If everything seems correct but the issue persists, you can delete and recreate the pod to see if the volume mount issue resolves.

Delete the pod:

bash
Copy code
microk8s kubectl delete pod prometheus-server-5cf87687d7-tv5p2 -n monitoring
Monitor the pod status:

bash
Copy code
microk8s kubectl get pods -n monitoring
Once the pod is back, check again if the /data directory is accessible inside the container.

Let me know if any of these steps resolve the issue, or if you see something unusual in the output!

